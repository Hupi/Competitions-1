{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data\n",
    "data is generated from another script with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../tmp/processed_df_2017.csv', parse_dates=['transactiondate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load feature names\n",
    "feats_numeric = pickle.load(open('../tmp/feats_numeric.pkl', 'r'))\n",
    "feats_categorical = pickle.load(open('../tmp/feats_categorical.pkl', 'r'))\n",
    "feats = pickle.load(open('../tmp/feats.pkl', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train/validation/prediction labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = pickle.load(open('../tmp/mask_train.pkl', 'r'))\n",
    "mask_test = pickle.load(open('../tmp/mask_validation.pkl', 'r'))\n",
    "mask_prediction = pickle.load(open('../tmp/mask_prediction.pkl', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to impute all these with a `missing` value first\n",
    "df[feats_categorical] = df[feats_categorical].fillna('--unknown--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use label encoder to encode all categorical variables\n",
    "le = LabelEncoder()\n",
    "for feat in feats_categorical:\n",
    "    df[[feat]] = df[[feat]].apply(le.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp =  Imputer(missing_values=np.nan, strategy=\"median\", axis=0)\n",
    "# df[feats_numeric] = imp.fit_transform(df[feats_numeric])\n",
    "for feat in feats_numeric:\n",
    "    if df[feat].isnull().sum() < df.shape[0]:\n",
    "        df[[feat]] = imp.fit_transform(df[[feat]])\n",
    "    else:\n",
    "        df[feat] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate training testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some extra filtering to remove outliers\n",
    "# X_train = df.loc[mask_train, feats].astype(float).values\n",
    "# X_test = df.loc[mask_test, feats].astype(float).values\n",
    "\n",
    "X_train = df.loc[(mask_train & (df.logerror<=0.419) & (df.logerror>=-0.4)), feats].astype(float).values\n",
    "X_test = df.loc[mask_test, feats].astype(float).values\n",
    "\n",
    "y_train = np.array(df.loc[(mask_train & (df.logerror<=0.419) & (df.logerror>=-0.4)), 'logerror'].tolist())\n",
    "y_test = np.array(df.loc[mask_test, 'logerror'].tolist())\n",
    "\n",
    "X_pred = df.loc[mask_prediction, feats].astype(float).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['max_bin'] = 10\n",
    "params['learning_rate'] = 0.0021 # shrinkage_rate\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'mae'          # or 'mae'\n",
    "params['sub_feature'] = 0.345    \n",
    "params['bagging_fraction'] = 0.85 # sub_row\n",
    "params['bagging_freq'] = 40\n",
    "params['num_leaves'] = 512        # num_leaf\n",
    "params['min_data'] = 500         # min_data_in_leaf\n",
    "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n",
    "params['verbose'] = 0\n",
    "params['feature_fraction_seed'] = 2\n",
    "params['bagging_seed'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(X_train, label=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "fit = lgb.train(params, d_train, 430)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for test set and prediction set\n",
    "y_pred = fit.predict(X_test)\n",
    "pred_lgb = fit.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_test_set_performance():\n",
    "    #explained_variance_score(y_pred=y_pred, y_true=y_test)\n",
    "    print 'MAE:{}'.format(round(mean_absolute_error(y_pred=y_pred, y_true=y_test),6))\n",
    "    # pd.Series(pred_lgb_test).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.073243\n"
     ]
    }
   ],
   "source": [
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.073243\n"
     ]
    }
   ],
   "source": [
    "# when not using october's data\n",
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost params\n",
    "params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': np.mean(y_train),\n",
    "    'silent': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(X_train, y_train)\n",
    "d_test = xgb.DMatrix(X_test)\n",
    "d_pred = xgb.DMatrix(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_rounds = 150\n",
    "fit = xgb.train(dict(params, silent=1), d_train, num_boost_round=num_boost_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for test set and prediction set\n",
    "y_pred = fit.predict(d_test)\n",
    "pred_xgb = fit.predict(d_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.076796\n"
     ]
    }
   ],
   "source": [
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.076796\n"
     ]
    }
   ],
   "source": [
    "# when not using oct data\n",
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\": 10,\n",
    "    \"min_samples_split\": 50,\n",
    "    \"n_estimators\": 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   29.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=10,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=50,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
       "           oob_score=False, random_state=42, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit = RandomForestRegressor(random_state=42, n_jobs=-1, verbose=1, **params)\n",
    "fit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   14.9s finished\n"
     ]
    }
   ],
   "source": [
    "# prediction for test set and prediction set\n",
    "y_pred = fit.predict(X_test)\n",
    "pred_rf = fit.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.076814\n"
     ]
    }
   ],
   "source": [
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.076814\n"
     ]
    }
   ],
   "source": [
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'l1_ratio':0.7, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.7,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit = ElasticNet(**params)\n",
    "fit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for test set and prediction set\n",
    "y_pred = fit.predict(X_test)\n",
    "pred_enet = fit.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.073539\n"
     ]
    }
   ],
   "source": [
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.073539\n"
     ]
    }
   ],
   "source": [
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensenmble - bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = {\n",
    "    'light_gbm': 1,\n",
    "    'xgboost': 0.00,\n",
    "    'random_forest': 0.00,\n",
    "    'elastic_net': 0.1,\n",
    "    'baseline': 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all = \\\n",
    "    (pred_lgb * model_weights['light_gbm'] + \\\n",
    "    pred_xgb * model_weights['xgboost'] +\\\n",
    "    pred_rf * model_weights['random_forest'] +\\\n",
    "    pred_enet * model_weights['elastic_net'] +\\\n",
    "    df.logerror.mean() * model_weights['baseline']) / sum(model_weights.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = df[['parcelid','logerror']]\n",
    "df_submission['pred'] = pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_submission['201610'] = df_submission['logerror'].combine_first(df_submission['pred'])\n",
    "df_submission['201610'] = df_submission['pred']\n",
    "df_submission['201611'] = df_submission['pred']\n",
    "df_submission['201612'] = df_submission['pred']\n",
    "df_submission['201710'] = df_submission['pred']\n",
    "df_submission['201711'] = df_submission['pred']\n",
    "df_submission['201712'] = df_submission['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.drop_duplicates(subset = ['parcelid'], keep='first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_submission['logerror']\n",
    "del df_submission['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.shape[0] == 2985217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('../tmp/submission_{}.csv'.format(datetime.datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
