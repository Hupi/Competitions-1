{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properties_2016 = pd.read_csv('../data/properties_2016.csv')\n",
    "df_properties_2017 = pd.read_csv('../data/properties_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions_2016 = pd.read_csv('../data/train_2016_v2.csv')\n",
    "df_transactions_2017 = pd.read_csv('../data/train_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../data/sample_submission.csv', low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_2016 = pd.merge(df_transactions_2016, df_properties_2016, how = 'left', on = 'parcelid')\n",
    "df_train_2017 = pd.merge(df_transactions_2017, df_properties_2017, how = 'left', on = 'parcelid')\n",
    "\n",
    "# assign 2017 tax data to NULL due to info leak\n",
    "# df_train_2017[['structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxvaluedollarcnt', 'taxamount']] = np.nan\n",
    "\n",
    "# merge the two set \n",
    "df_train = pd.concat([df_train_2016, df_train_2017], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_2016 = pd.merge(sample_submission[['ParcelId']], df_properties_2016.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                how = 'left', on = 'ParcelId')\n",
    "df_pred_2017 = pd.merge(sample_submission[['ParcelId']], df_properties_2017.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                how = 'left', on = 'ParcelId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get some memory back\n",
    "del df_properties_2016, df_properties_2017, df_train_2016, df_train_2017\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>avg_logerror</th>\n",
       "      <th>avg_abs_logerror</th>\n",
       "      <th>std_dev_logerror</th>\n",
       "      <th>std_dev_abs_logerror</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.019012</td>\n",
       "      <td>0.073773</td>\n",
       "      <td>0.172725</td>\n",
       "      <td>0.157330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.018463</td>\n",
       "      <td>0.074905</td>\n",
       "      <td>0.181878</td>\n",
       "      <td>0.166761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.012199</td>\n",
       "      <td>0.070451</td>\n",
       "      <td>0.163923</td>\n",
       "      <td>0.148513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.008849</td>\n",
       "      <td>0.069463</td>\n",
       "      <td>0.162658</td>\n",
       "      <td>0.147345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.009161</td>\n",
       "      <td>0.067088</td>\n",
       "      <td>0.157046</td>\n",
       "      <td>0.142290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  avg_logerror  avg_abs_logerror  std_dev_logerror  \\\n",
       "0      1      0.019012          0.073773          0.172725   \n",
       "1      2      0.018463          0.074905          0.181878   \n",
       "2      3      0.012199          0.070451          0.163923   \n",
       "3      4      0.008849          0.069463          0.162658   \n",
       "4      5      0.009161          0.067088          0.157046   \n",
       "\n",
       "   std_dev_abs_logerror  \n",
       "0              0.157330  \n",
       "1              0.166761  \n",
       "2              0.148513  \n",
       "3              0.147345  \n",
       "4              0.142290  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features apply to all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date_features(df):\n",
    "    #df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n",
    "    #df[\"transaction_month\"] = (df[\"transactiondate\"].dt.year - 2016)*12 + df[\"transactiondate\"].dt.month\n",
    "    \n",
    "    df['transaction_month'] = pd.to_datetime(t.transactiondate).dt.month\n",
    "    df[\"transaction_quarter\"] = pd.to_datetime(t.transactiondate).dt.quarter\n",
    "    \n",
    "    # df.drop([\"transactiondate\"], inplace=True, axis=1)\n",
    "\n",
    "    #error in calculation of the finished living area of home\n",
    "    df['f_num_LivingAreaError'] = df['calculatedfinishedsquarefeet']/df['finishedsquarefeet12']\n",
    "\n",
    "    #proportion of living area\n",
    "    df['f_num_LivingAreaProp'] = df['calculatedfinishedsquarefeet']/df['lotsizesquarefeet']\n",
    "    df['f_num_LivingAreaProp2'] = df['finishedsquarefeet12']/df['finishedsquarefeet15']\n",
    "\n",
    "    #Amout of extra space\n",
    "    df['f_num_ExtraSpace'] = df['lotsizesquarefeet'] - df['calculatedfinishedsquarefeet'] \n",
    "    df['f_num_ExtraSpace-2'] = df['finishedsquarefeet15'] - df['finishedsquarefeet12'] \n",
    "\n",
    "    #Total number of rooms\n",
    "    df['f_num_TotalRooms'] = df['bathroomcnt']*df['bedroomcnt']\n",
    "\n",
    "    #Average room size\n",
    "    df['f_num_AvRoomSize'] = df['calculatedfinishedsquarefeet']/df['roomcnt'] \n",
    "\n",
    "    # Number of Extra rooms\n",
    "    df['f_num_ExtraRooms'] = df['roomcnt'] - df['f_num_TotalRooms'] \n",
    "\n",
    "    #Ratio of the built structure value to land area\n",
    "    df['f_num_ValueProp'] = df['structuretaxvaluedollarcnt']/df['landtaxvaluedollarcnt']\n",
    "\n",
    "    #Does property have a garage, pool or hot tub and AC?\n",
    "    df['f_num_GarPoolAC'] = ((df['garagecarcnt']>0) & (df['pooltypeid10']>0) & (df['airconditioningtypeid']!=5))*1 \n",
    "\n",
    "    df[\"f_num_location\"] = df[\"latitude\"] + df[\"longitude\"]\n",
    "    df[\"f_num_locatiof_num_2\"] = df[\"latitude\"]*df[\"longitude\"]\n",
    "    df[\"f_num_locatiof_num_2round\"] = df[\"f_num_locatiof_num_2\"].round(-4)\n",
    "\n",
    "    df[\"f_num_latitude-round\"] = df[\"latitude\"].round(-4)\n",
    "    df[\"f_num_longitude-round\"] = df[\"longitude\"].round(-4)\n",
    "    \n",
    "    #polnomials of the variable\n",
    "    df[\"f_num_structuretaxvaluedollarcnt-2\"] = df[\"structuretaxvaluedollarcnt\"] ** 2\n",
    "    df[\"f_num_structuretaxvaluedollarcnt-3\"] = df[\"structuretaxvaluedollarcnt\"] ** 3\n",
    "\n",
    "    #Ratio of tax of property over parcel\n",
    "    df['ValueRatio'] = df['taxvaluedollarcnt']/df['taxamount']\n",
    "\n",
    "    #TotalTaxScore\n",
    "    df['TaxScore'] = df['taxvaluedollarcnt']*df['taxamount']\n",
    "\n",
    "    #polnomials of tax delinquency year\n",
    "    df[\"taxdelinquencyyear-2\"] = df[\"taxdelinquencyyear\"] ** 2\n",
    "    df[\"taxdelinquencyyear-3\"] = df[\"taxdelinquencyyear\"] ** 3\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_2016 = add_date_features(df_train_2016)\n",
    "df_train_2017 = add_date_features(df_train_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_2016 = add_date_features(df_pred_2016)\n",
    "df_pred_2017 = add_date_features(df_pred_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### needs to be generated from full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_location_variables = ['regionidneighborhood',\n",
    "                                 'regionidzip', \n",
    "                                 'regionidcity', \n",
    "                                 #'censustrack',\n",
    "                                 'rawcensustractandblock']\n",
    "\n",
    "# through for all 2016 data:\n",
    "for region in parcel_location_variables:\n",
    "\n",
    "    #### COUNT OF PROPERTIES ####\n",
    "    # number of properties in the zipcode\n",
    "    group = df_pred_2016[region].value_counts().to_dict()\n",
    "    df_pred_2016['f_num_n_prop_in_'+region] = df_pred_2016[region].map(group)\n",
    "    df_train_2016['f_num_n_prop_in_'+region] = df_train_2016[region].map(group)\n",
    "    \n",
    "    ##### HOW NEW IS THIS BUILDING COMPARING TO OTHER BUILDINGS #####\n",
    "    group = df_pred_2016.groupby(region)['yearbuilt'].aggregate('median').to_dict()\n",
    "    df_pred_2016['f_cat_median_year_in_'+region] = df_pred_2016[region].map(group)\n",
    "    df_train_2016['f_cat_median_year_in_'+region] = df_train_2016[region].map(group)\n",
    "    \n",
    "    df_pred_2016['f_num_how_new_in_'+region] = df_pred_2016['yearbuilt'] - df_pred_2016['f_cat_median_year_in_'+region]\n",
    "    df_train_2016['f_num_how_new_in_'+region] = df_train_2016['yearbuilt'] - df_train_2016['f_cat_median_year_in_'+region]\n",
    "\n",
    "    # Neighborhood latitude and longitude\n",
    "    group = df_pred_2016.groupby(region)['latitude'].aggregate('median').to_dict()\n",
    "    df_pred_2016['f_num_median_lat_in_'+region] = df_pred_2016[region].map(group)\n",
    "    df_train_2016['f_num_median_lat_in_'+region] = df_train_2016[region].map(group)\n",
    "    \n",
    "    \n",
    "    group = df_pred_2016.groupby(region)['longitude'].aggregate('median').to_dict()\n",
    "    df_pred_2016['f_num_median_lon_in_'+region] = df_pred_2016[region].map(group)\n",
    "    df_train_2016['f_num_median_lon_in_'+region] = df_train_2016[region].map(group)\n",
    "\n",
    "    #Average structuretaxvaluedollarcnt by city\n",
    "    group = df_pred_2016.groupby(region)['structuretaxvaluedollarcnt'].aggregate('mean').to_dict()\n",
    "    df_pred_2016['f_num_Avg-structuretaxvaluedollarcnt'] = df_pred_2016['regionidcity'].map(group)\n",
    "    df_train_2016['f_num_Avg-structuretaxvaluedollarcnt'] = df_train_2016['regionidcity'].map(group)\n",
    "\n",
    "    #Deviation away from average\n",
    "    df_pred_2016['f_num_Dev-structuretaxvaluedollarcnt'] = abs((df_pred_2016['structuretaxvaluedollarcnt'] - df_pred_2016['f_num_Avg-structuretaxvaluedollarcnt']))/df_pred_2016['f_num_Avg-structuretaxvaluedollarcnt']\n",
    "    df_train_2016['f_num_Dev-structuretaxvaluedollarcnt'] = abs((df_train_2016['structuretaxvaluedollarcnt'] - df_train_2016['f_num_Avg-structuretaxvaluedollarcnt']))/df_train_2016['f_num_Avg-structuretaxvaluedollarcnt']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and do it for 2017\n",
    "for region in parcel_location_variables:\n",
    "\n",
    "    #### COUNT OF PROPERTIES ####\n",
    "    # number of properties in the zipcode\n",
    "    group = df_pred_2017[region].value_counts().to_dict()\n",
    "    df_pred_2017['f_num_n_prop_in_'+region] = df_pred_2017[region].map(group)\n",
    "    df_train_2017['f_num_n_prop_in_'+region] = df_train_2017[region].map(group)\n",
    "    \n",
    "    ##### HOW NEW IS THIS BUILDING COMPARING TO OTHER BUILDINGS #####\n",
    "    group = df_pred_2017.groupby(region)['yearbuilt'].aggregate('median').to_dict()\n",
    "    df_pred_2017['f_cat_median_year_in_'+region] = df_pred_2017[region].map(group)\n",
    "    df_train_2017['f_cat_median_year_in_'+region] = df_train_2017[region].map(group)\n",
    "    \n",
    "    df_pred_2017['f_num_how_new_in_'+region] = df_pred_2017['yearbuilt'] - df_pred_2017['f_cat_median_year_in_'+region]\n",
    "    df_train_2017['f_num_how_new_in_'+region] = df_train_2017['yearbuilt'] - df_train_2017['f_cat_median_year_in_'+region]\n",
    "\n",
    "    # Neighborhood latitude and longitude\n",
    "    group = df_pred_2017.groupby(region)['latitude'].aggregate('median').to_dict()\n",
    "    df_pred_2017['f_num_median_lat_in_'+region] = df_pred_2017[region].map(group)\n",
    "    df_train_2017['f_num_median_lat_in_'+region] = df_train_2017[region].map(group)\n",
    "    \n",
    "    \n",
    "    group = df_pred_2017.groupby(region)['longitude'].aggregate('median').to_dict()\n",
    "    df_pred_2017['f_num_median_lon_in_'+region] = df_pred_2017[region].map(group)\n",
    "    df_train_2017['f_num_median_lon_in_'+region] = df_train_2017[region].map(group)\n",
    "\n",
    "    #Average structuretaxvaluedollarcnt by city\n",
    "    group = df_pred_2017.groupby(region)['structuretaxvaluedollarcnt'].aggregate('mean').to_dict()\n",
    "    df_pred_2017['f_num_Avg-structuretaxvaluedollarcnt'] = df_pred_2017['regionidcity'].map(group)\n",
    "    df_train_2017['f_num_Avg-structuretaxvaluedollarcnt'] = df_train_2017['regionidcity'].map(group)\n",
    "\n",
    "    #Deviation away from average\n",
    "    df_pred_2017['f_num_Dev-structuretaxvaluedollarcnt'] = abs((df_pred_2017['structuretaxvaluedollarcnt'] - df_pred_2017['f_num_Avg-structuretaxvaluedollarcnt']))/df_pred_2017['f_num_Avg-structuretaxvaluedollarcnt']\n",
    "    df_train_2017['f_num_Dev-structuretaxvaluedollarcnt'] = abs((df_train_2017['structuretaxvaluedollarcnt'] - df_train_2017['f_num_Avg-structuretaxvaluedollarcnt']))/df_train_2017['f_num_Avg-structuretaxvaluedollarcnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now additional features only applicable to transaction data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. time related features\n",
    "Here the features are generated from SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS transactions_2016;\n",
      "DROP TABLE\n",
      "CREATE TABLE transactions_2016 (\n",
      "\tparcelid bigint, \n",
      "\tlogerror double precision,\n",
      "\ttransactiondate varchar\n",
      ");\n",
      "CREATE TABLE\n",
      "DROP TABLE IF EXISTS transactions_2017;\n",
      "DROP TABLE\n",
      "COPY transactions_2016\n",
      "FROM '/Users/dai_li/Workspace/personal/Competitions/zillow/data/train_2016_v2.csv' DELIMITER ',' CSV HEADER;\n",
      "COPY 90275\n",
      "CREATE TABLE transactions_2017 (\n",
      "\tparcelid bigint, \n",
      "\tlogerror double precision,\n",
      "\ttransactiondate varchar\n",
      ");\n",
      "CREATE TABLE\n",
      "COPY transactions_2017\n",
      "FROM '/Users/dai_li/Workspace/personal/Competitions/zillow/data/train_2017.csv' DELIMITER ',' CSV HEADER;\n",
      "COPY 77613\n",
      "DROP TABLE IF EXISTS transactions;\n",
      "DROP TABLE\n",
      "CREATE TABLE transactions AS\n",
      "SELECT * FROM transactions_2016\n",
      "UNION ALL\n",
      "SELECT * FROM transactions_2017;\n",
      "SELECT 167888\n",
      "DROP TABLE IF EXISTS tmp_additional_temporal_information;\n",
      "DROP TABLE\n",
      "CREATE TABLE tmp_additional_temporal_information AS\n",
      "SELECT \n",
      "    t.parcelid\n",
      "    , t.logerror\n",
      "    , t.transactiondate\n",
      "    , substring(transactiondate from 1 for 4) AS year\n",
      "    , substring(transactiondate from 6 for 2) AS month\n",
      "    , DATE_PART('dow',date(t.transactiondate)) AS day_of_week\n",
      "    , substring(transactiondate from 1 for 7) AS year_and_month\n",
      "FROM \n",
      "\ttransactions t\n",
      ";\n",
      "SELECT 167888\n",
      "-- this table the logerror information by month\n",
      "DROP TABLE IF EXISTS tmp_aggregated_logerror_information;\n",
      "DROP TABLE\n",
      "CREATE TABLE tmp_aggregated_logerror_information AS\n",
      "SELECT\n",
      "\tmonth\n",
      "\t, AVG(logerror) avg_logerror\n",
      "\t, AVG(abs(logerror)) avg_abs_logerror\n",
      "\t, STDDEV(logerror) std_dev_logerror\n",
      "\t, STDDEV(abs(logerror)) std_dev_abs_logerror \n",
      "FROM\n",
      "\ttmp_additional_temporal_information\n",
      "GROUP BY\n",
      "\t1\n",
      "ORDER BY\n",
      "\t1 ASC\n",
      ";\n",
      "SELECT 12\n",
      "COPY tmp_aggregated_logerror_information TO '/Users/dai_li/Workspace/personal/Competitions/zillow/data/2nd_monthly_transactions_features.csv' DELIMITER ',' CSV HEADER;\n",
      "COPY 12\n",
      "-- DROP TABLE IF EXISTS tmp_aggregated_logerror_infomation_and_trend;\n",
      "-- CREATE TABLE tmp_aggregated_logerror_infomation_and_trend AS\n",
      "-- SELECT\n",
      "-- \tyear_and_month\n",
      "-- \t, avg_logerror\n",
      "-- \t, avg_abs_logerror\n",
      "-- \t, std_dev_logerror\n",
      "-- \t, std_dev_abs_logerror\n",
      "\t\n",
      "-- \t, AVG(avg_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) avg_logerror_last_1_month\n",
      "-- \t, AVG(avg_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 2 PRECEDING AND 1 PRECEDING) avg_logerror_last_2_month\n",
      "-- \t, AVG(avg_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING) avg_logerror_last_3_month\n",
      "-- \t, AVG(std_dev_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) avg_std_dev_logerror_last_1_month\n",
      "-- \t, AVG(std_dev_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 2 PRECEDING AND 1 PRECEDING) avg_std_dev_logerror_last_2_month\n",
      "-- \t, AVG(std_dev_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING) avg_std_dev_logerror_last_3_month\n",
      "-- \t, AVG(std_dev_abs_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) avg_std_dev_abs_logerror_last_1_month\n",
      "-- \t, AVG(std_dev_abs_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 2 PRECEDING AND 1 PRECEDING) avg_std_dev_abs_logerror_last_2_month\n",
      "-- \t, AVG(std_dev_abs_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING) avg_std_dev_abs_logerror_last_3_month\n",
      "-- FROM\n",
      "-- \ttmp_aggregated_logerror_information\n",
      "-- ;\n",
      "-- COPY tmp_aggregated_logerror_infomation_and_trend TO '/Users/dai_li/Workspace/personal/Competitions/zillow/data/monthly_transactions_features.csv' DELIMITER ',' CSV HEADER;\n",
      "-- -- this table generates additional features for transactions\n",
      "-- DROP TABLE IF EXISTS transactions_additional_features;\n",
      "-- CREATE TABLE transactions_additional_features as\n",
      "-- SELECT \n",
      "-- \tt.parcelid\n",
      "--     , t.logerror\n",
      "--     , t.transactiondate\n",
      "--     , t.year\n",
      "--     , t.month\n",
      "--     , t.day_of_week\n",
      "--     , f.*\n",
      "-- FROM \n",
      "-- \ttmp_additional_temporal_information t\n",
      "-- JOIN\n",
      "-- \ttmp_aggregated_logerror_infomation_and_trend f\n",
      "-- ON\n",
      "-- \tt.year_and_month = f.year_and_month\t\t\n",
      "-- ;\n",
      "-- COPY transactions_additional_features TO '/Users/dai_li/Workspace/personal/Competitions/zillow/data/transactions_features.csv' DELIMITER ',' CSV HEADER;\n"
     ]
    }
   ],
   "source": [
    "!psql -d zillow -a -f ../scripts/feature_engineering_time.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_features = pd.read_csv('../data/2nd_monthly_transactions_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train_2016, df_train_2017], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['f_num_month'] = pd.to_datetime(df_train.transactiondate).dt.month.astype(int)\n",
    "df_train['f_num_quarter'] = pd.to_datetime(df_train.transactiondate).dt.quarter.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(4)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in [201610, 201611, 201612, 201710, 201711, 201712]:\n",
    "    year = str(m)[:4]\n",
    "    month = str(m)[4:]\n",
    "    \n",
    "    # avg log error, monthly\n",
    "    group = df_train.groupby(df_train.f_num_month)['logerror'].aggregate('mean').to_dict()\n",
    "    df_train['f_num_monthly_avg_logerror'] = df_train['f_num_month'].map(group)\n",
    "    eval('df_pred_'+str(m))['f_num_monthly_avg_logerror'] = group[int(month)]\n",
    "                                   \n",
    "    # std dev log error, monthly\n",
    "    group = df_train.groupby(df_train.f_num_month)['logerror'].aggregate('std').to_dict()\n",
    "    df_train['f_num_monthly_stddev_logerror'] = df_train['f_num_month'].map(group)\n",
    "    eval('df_pred_'+str(m))['f_num_monthly_stddev_logerror'] = group[int(month)]\n",
    "\n",
    "    # avg log error, quarterly\n",
    "    group = df_train.groupby(df_train.f_num_month)['logerror'].aggregate('mean').to_dict()\n",
    "    df_train['f_num_quarterly_avg_logerror'] = df_train['f_num_quarter'].map(group)\n",
    "    eval('df_pred_'+str(m))['f_num_quarterly_avg_logerror'] = group[4]\n",
    "    \n",
    "    # std dev log error, monthly\n",
    "    group = df_train.groupby(df_train.f_num_month)['logerror'].aggregate('std').to_dict()\n",
    "    df_train['f_num_quarterly_stddev_logerror'] = df_train['f_num_quarter'].map(group)\n",
    "    eval('df_pred_'+str(m))['f_num_quarterly_stddev_logerror'] = group[4]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the cleaned data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../tmp/train_full.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_2016.to_csv('../tmp/pred_2016.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_2017.to_csv('../tmp/pred_2017.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fields = set(df.columns)\n",
    "\n",
    "# these are fields that are used to identify fields\n",
    "identifiers = set(['transactiondate', 'parcelid'])\n",
    "\n",
    "# log error that we want to model\n",
    "label = set(['logerror'])\n",
    "\n",
    "# the following are categorical features\n",
    "feats_objects = set(\n",
    " ['taxdelinquencyflag',\n",
    " 'propertycountylandusecode',\n",
    " 'propertyzoningdesc',\n",
    " 'fireplaceflag',\n",
    " 'hashottuborspa']\n",
    ")\n",
    "\n",
    "# the following are numerical features that should be treated as categorical features\n",
    "feats_categorical_as_numeric = set([\n",
    "    'airconditioningtypeid',\n",
    "    'architecturalstyletypeid',\n",
    "    'buildingqualitytypeid',\n",
    "    'buildingclasstypeid',\n",
    "    'decktypeid',\n",
    "    'fips',\n",
    "    'heatingorsystemtypeid',\n",
    "    'propertylandusetypeid',\n",
    "    'regionidcounty',\n",
    "    'regionidcity',\n",
    "    'regionidzip',\n",
    "    'regionidneighborhood',\n",
    "    'storytypeid',\n",
    "    'typeconstructiontypeid',\n",
    "])\n",
    "\n",
    "\n",
    "# the rest are numeric features\n",
    "feats_numeric = set([\n",
    "    'basementsqft',\n",
    "    'bathroomcnt',\n",
    "    'bedroomcnt',\n",
    "    'calculatedbathnbr',\n",
    "    'threequarterbathnbr',\n",
    "    'finishedfloor1squarefeet',\n",
    "    'calculatedfinishedsquarefeet',\n",
    "    'finishedsquarefeet6',\n",
    "    'finishedsquarefeet12',\n",
    "    'finishedsquarefeet13',\n",
    "    'finishedsquarefeet15',\n",
    "    'finishedsquarefeet50',\n",
    "    'fireplacecnt',\n",
    "    'fullbathcnt',\n",
    "    'garagecarcnt',\n",
    "    'garagetotalsqft',\n",
    "    'hashottuborspa',\n",
    "    'lotsizesquarefeet',\n",
    "    'numberofstories',\n",
    "    'poolcnt',\n",
    "    'poolsizesum',\n",
    "    'pooltypeid10',\n",
    "    'pooltypeid2',\n",
    "    'pooltypeid7',\n",
    "    'roomcnt',\n",
    "    'unitcnt',\n",
    "    'yardbuildingsqft17',\n",
    "    'yardbuildingsqft26',\n",
    "    'taxvaluedollarcnt',\n",
    "    'structuretaxvaluedollarcnt',\n",
    "    'landtaxvaluedollarcnt',\n",
    "    'taxamount',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'yearbuilt',\n",
    "    'assessmentyear',\n",
    "    'taxdelinquencyyear',\n",
    "    'rawcensustractandblock',\n",
    "    'censustractandblock',\n",
    "])\n",
    "\n",
    "feats_numerics_feature_engineered = set([col for col in df.columns if 'f_num' in col or '_logerror' in col])\n",
    "\n",
    "\n",
    "feats_categorical_feature_engineered = set([col for col in df.columns if 'f_cat' in col])\n",
    "\n",
    "\n",
    "# fields that are thrown away for now\n",
    "feats_for_consideration_later = set([\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_categorical = feats_objects | feats_categorical_feature_engineered\n",
    "feats_numeric = feats_numeric | feats_numerics_feature_engineered \n",
    "feats = feats_categorical | feats_numeric | feats_categorical_as_numeric\n",
    "\n",
    "feats_categorical_as_numeric = list(feats_categorical_as_numeric)\n",
    "feats_categorical = list(feats_categorical)\n",
    "feats_numeric = list(feats_numeric)\n",
    "feats = list(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save results to pick files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(feats_categorical_as_numeric, open('../tmp/feats_categorical_as_numeric.pkl', 'w'))\n",
    "pickle.dump(feats_categorical, open('../tmp/feats_categorical.pkl', 'w'))\n",
    "pickle.dump(feats_numeric, open('../tmp/feats_numeric.pkl', 'w'))\n",
    "pickle.dump(feats, open('../tmp/feats.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = (df.transactiondate < '2019-01-01')\n",
    "mask_validation = (df.transactiondate >= '2016-10-01') & (df.transactiondate < '2017-01-01')\n",
    "# mask_prediction = ~df.parcelid.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mask_train, open('../tmp/mask_train.pkl', 'w'))\n",
    "pickle.dump(mask_validation, open('../tmp/mask_validation.pkl', 'w'))\n",
    "# pickle.dump(mask_prediction, open('../tmp/mask_prediction.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finishedfloor1squarefeet',\n",
       " 'numberofstories',\n",
       " 'f_num_longitude-round',\n",
       " 'f_cat_median_year_in_regionidneighborhood',\n",
       " 'f_num_locatiof_num_2round',\n",
       " 'regionidzip',\n",
       " 'f_num_avg_logerror',\n",
       " 'buildingclasstypeid',\n",
       " 'regionidcounty',\n",
       " 'taxdelinquencyflag',\n",
       " 'f_num_ValueProp',\n",
       " 'pooltypeid10',\n",
       " 'f_num_LivingAreaProp2',\n",
       " 'f_num_how_new_in_regionidcity',\n",
       " 'f_num_n_prop_in_regionidcity',\n",
       " 'structuretaxvaluedollarcnt',\n",
       " 'poolsizesum',\n",
       " 'f_num_n_prop_in_rawcensustractandblock',\n",
       " 'f_num_median_lon_in_regionidneighborhood',\n",
       " 'finishedsquarefeet6',\n",
       " 'f_num_monthly_stddev_logerror',\n",
       " 'calculatedfinishedsquarefeet',\n",
       " 'latitude',\n",
       " 'bedroomcnt',\n",
       " 'landtaxvaluedollarcnt',\n",
       " 'hashottuborspa',\n",
       " 'f_num_locatiof_num_2',\n",
       " 'buildingqualitytypeid',\n",
       " 'f_num_median_lat_in_regionidcity',\n",
       " 'f_num_GarPoolAC',\n",
       " 'propertylandusetypeid',\n",
       " 'regionidneighborhood',\n",
       " 'f_num_stddev_logerror',\n",
       " 'decktypeid',\n",
       " 'f_cat_median_year_in_rawcensustractandblock',\n",
       " 'heatingorsystemtypeid',\n",
       " 'architecturalstyletypeid',\n",
       " 'rawcensustractandblock',\n",
       " 'f_num_quarter',\n",
       " 'f_num_median_lon_in_regionidzip',\n",
       " 'f_num_structuretaxvaluedollarcnt-3',\n",
       " 'f_num_structuretaxvaluedollarcnt-2',\n",
       " 'longitude',\n",
       " 'basementsqft',\n",
       " 'taxvaluedollarcnt',\n",
       " 'f_cat_median_year_in_regionidcity',\n",
       " 'f_num_n_prop_in_regionidzip',\n",
       " 'f_num_median_lon_in_regionidcity',\n",
       " 'f_num_quarterly_avg_logerror',\n",
       " 'fullbathcnt',\n",
       " 'fireplaceflag',\n",
       " 'poolcnt',\n",
       " 'typeconstructiontypeid',\n",
       " 'f_num_median_lat_in_regionidneighborhood',\n",
       " 'f_num_median_lat_in_regionidzip',\n",
       " 'yearbuilt',\n",
       " 'calculatedbathnbr',\n",
       " 'fireplacecnt',\n",
       " 'f_num_LivingAreaError',\n",
       " 'f_num_latitude-round',\n",
       " 'storytypeid',\n",
       " 'f_num_ExtraSpace-2',\n",
       " 'f_num_how_new_in_rawcensustractandblock',\n",
       " 'propertyzoningdesc',\n",
       " 'f_num_ExtraRooms',\n",
       " 'garagetotalsqft',\n",
       " 'f_num_monthly_avg_logerror',\n",
       " 'censustractandblock',\n",
       " 'taxdelinquencyyear',\n",
       " 'threequarterbathnbr',\n",
       " 'roomcnt',\n",
       " 'regionidcity',\n",
       " 'yardbuildingsqft17',\n",
       " 'f_num_LivingAreaProp',\n",
       " 'f_cat_median_year_in_regionidzip',\n",
       " 'taxamount',\n",
       " 'f_num_quarterly_stddev_logerror',\n",
       " 'unitcnt',\n",
       " 'f_num_location',\n",
       " 'assessmentyear',\n",
       " 'f_num_how_new_in_regionidneighborhood',\n",
       " 'propertycountylandusecode',\n",
       " 'f_num_month',\n",
       " 'bathroomcnt',\n",
       " 'f_num_ExtraSpace',\n",
       " 'f_num_median_lat_in_rawcensustractandblock',\n",
       " 'f_num_n_prop_in_regionidneighborhood',\n",
       " 'airconditioningtypeid',\n",
       " 'garagecarcnt',\n",
       " 'f_num_Dev-structuretaxvaluedollarcnt',\n",
       " 'f_num_AvRoomSize',\n",
       " 'pooltypeid2',\n",
       " 'finishedsquarefeet50',\n",
       " 'f_num_how_new_in_regionidzip',\n",
       " 'pooltypeid7',\n",
       " 'f_num_TotalRooms',\n",
       " 'finishedsquarefeet12',\n",
       " 'finishedsquarefeet13',\n",
       " 'yardbuildingsqft26',\n",
       " 'finishedsquarefeet15',\n",
       " 'lotsizesquarefeet',\n",
       " 'f_num_Avg-structuretaxvaluedollarcnt',\n",
       " 'fips',\n",
       " 'f_num_median_lon_in_rawcensustractandblock']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
