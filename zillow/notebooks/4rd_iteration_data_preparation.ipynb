{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_properties_2016 = pd.read_csv('../data/properties_2016.csv')\n",
    "df_properties_2017 = pd.read_csv('../data/properties_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions_2016 = pd.read_csv('../data/train_2016_v2.csv')\n",
    "df_transactions_2017 = pd.read_csv('../data/train_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('../data/sample_submission.csv', low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_2016 = pd.merge(df_transactions_2016, df_properties_2016, how = 'left', on = 'parcelid')\n",
    "df_train_2017 = pd.merge(df_transactions_2017, df_properties_2017, how = 'left', on = 'parcelid')\n",
    "\n",
    "# assign 2017 tax data to NULL due to info leak\n",
    "# df_train_2017[['structuretaxvaluedollarcnt', 'landtaxvaluedollarcnt', 'taxvaluedollarcnt', 'taxamount']] = np.nan\n",
    "\n",
    "# merge the two set \n",
    "df_train = pd.concat([df_train_2016, df_train_2017], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_2016 = pd.merge(sample_submission[['ParcelId']], df_properties_2016.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                how = 'left', on = 'ParcelId')\n",
    "df_pred_2017 = pd.merge(sample_submission[['ParcelId']], df_properties_2017.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "                how = 'left', on = 'ParcelId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get some memory back\n",
    "del df_properties_2016, df_properties_2017, df_train_2016, df_train_2017\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all features generated from the sql file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add censustract\n",
    "# t = pd.DataFrame(df['rawcensustractandblock'].astype(str).str.split('.',1).tolist(),columns = ['censustrack','censusblock'])\n",
    "# df['censustrack'] = t['censustrack']\n",
    "\n",
    "# t = pd.DataFrame(df_properties['rawcensustractandblock'].astype(str).str.split('.',1).tolist(),columns = ['censustrack','censusblock'])\n",
    "# df_properties['censustrack'] = t['censustrack']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.location related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(df):\n",
    "    ###################################\n",
    "    ####### DIMENSION: SPACE ##########\n",
    "    ###################################\n",
    "\n",
    "    parcel_location_variables = ['regionidneighborhood',\n",
    "                                 'regionidzip', \n",
    "                                 'regionidcity', \n",
    "                                 #'censustrack',\n",
    "                                 'rawcensustractandblock']\n",
    "\n",
    "    # iterate through all above regions\n",
    "    for region in parcel_location_variables:\n",
    "\n",
    "        #### COUNT OF PROPERTIES ####\n",
    "        # number of properties in the zipcode\n",
    "        df['f_num_n_prop_in_'+region] = df[region].map(df[region].value_counts().to_dict())\n",
    "\n",
    "        ##### HOW NEW IS THIS BUILDING COMPARING TO OTHER BUILDINGS #####\n",
    "        df['f_cat_median_year_in_'+region] = df[region].map(df.groupby(region)['yearbuilt'].aggregate('median').to_dict())\n",
    "        df['f_num_how_new_in_'+region] = df['yearbuilt'] - df['f_cat_median_year_in_'+region]\n",
    "\n",
    "        # Neighborhood latitude and longitude\n",
    "        df['f_num_median_lat_in_'+region] = df[region].map(df.groupby(region)['latitude'].aggregate('median').to_dict())\n",
    "        df['f_num_median_lon_in_'+region] = df[region].map(df.groupby(region)['longitude'].aggregate('median').to_dict())\n",
    "\n",
    "#         #### TRANSACTION RELATED ####\n",
    "#         # how many transaction made in this region per observed properties\n",
    "#         # this tells us how active a region is\n",
    "#         df['f_num_pct_trans_in_'+region] = df.groupby(region)['logerror'].aggregate('count') * 1.0 / df['f_num_n_prop_in_'+region] \n",
    "\n",
    "#         #### LOG ERROR RELATED ####\n",
    "#         df['f_num_error_std_in_'+region] = df[region].map(df[df.transactiondate < '2016-10-01'].groupby(region)['logerror'].aggregate(\"std\").to_dict())\n",
    "#         df['f_num_error_mean_in_'+region] = df[region].map(df[df.transactiondate < '2016-10-01'].groupby(region)['logerror'].aggregate(\"mean\").to_dict())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. time related features\n",
    "Here the features are generated from SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP TABLE IF EXISTS transactions_2016;\n",
      "DROP TABLE\n",
      "CREATE TABLE transactions_2016 (\n",
      "\tparcelid bigint, \n",
      "\tlogerror double precision,\n",
      "\ttransactiondate varchar\n",
      ");\n",
      "CREATE TABLE\n",
      "DROP TABLE IF EXISTS transactions_2017;\n",
      "DROP TABLE\n",
      "COPY transactions_2016\n",
      "FROM '/Users/dai_li/Workspace/personal/Competitions/zillow/data/train_2016_v2.csv' DELIMITER ',' CSV HEADER;\n",
      "COPY 90275\n",
      "CREATE TABLE transactions_2017 (\n",
      "\tparcelid bigint, \n",
      "\tlogerror double precision,\n",
      "\ttransactiondate varchar\n",
      ");\n",
      "CREATE TABLE\n",
      "COPY transactions_2017\n",
      "FROM '/Users/dai_li/Workspace/personal/Competitions/zillow/data/train_2017.csv' DELIMITER ',' CSV HEADER;\n",
      "COPY 77613\n",
      "DROP TABLE IF EXISTS transactions;\n",
      "DROP TABLE\n",
      "CREATE TABLE transactions AS\n",
      "SELECT * FROM transactions_2016\n",
      "UNION ALL\n",
      "SELECT * FROM transactions_2017;\n",
      "SELECT 167888\n",
      "DROP TABLE IF EXISTS tmp_additional_temporal_information;\n",
      "DROP TABLE\n",
      "CREATE TABLE tmp_additional_temporal_information AS\n",
      "SELECT \n",
      "    t.parcelid\n",
      "    , t.logerror\n",
      "    , t.transactiondate\n",
      "    , substring(transactiondate from 1 for 4) AS year\n",
      "    , substring(transactiondate from 5 for 2) AS month\n",
      "    , DATE_PART('dow',date(t.transactiondate)) AS day_of_week\n",
      "    , substring(transactiondate from 1 for 7) AS year_and_month\n",
      "FROM \n",
      "\ttransactions t\n",
      ";\n",
      "SELECT 167888\n",
      "-- this table the logerror information by month\n",
      "DROP TABLE IF EXISTS tmp_aggregated_logerror_information;\n",
      "DROP TABLE\n",
      "CREATE TABLE tmp_aggregated_logerror_information AS\n",
      "SELECT\n",
      "\tyear_and_month\n",
      "\t, AVG(logerror) avg_logerror\n",
      "\t, AVG(abs(logerror)) avg_abs_logerror\n",
      "\t, STDDEV(logerror) std_dev_logerror\n",
      "\t, STDDEV(abs(logerror)) std_dev_abs_logerror \n",
      "FROM\n",
      "\ttmp_additional_temporal_information\n",
      "GROUP BY\n",
      "\t1\n",
      "ORDER BY\n",
      "\t1 ASC\n",
      ";\n",
      "SELECT 21\n",
      "DROP TABLE IF EXISTS tmp_aggregated_logerror_infomation_and_trend;\n",
      "DROP TABLE\n",
      "CREATE TABLE tmp_aggregated_logerror_infomation_and_trend AS\n",
      "SELECT\n",
      "\tyear_and_month\n",
      "\t, avg_logerror\n",
      "\t, avg_abs_logerror\n",
      "\t, std_dev_logerror\n",
      "\t, std_dev_abs_logerror\n",
      "\t\n",
      "\t, AVG(avg_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) avg_logerror_last_1_month\n",
      "\t, AVG(avg_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 2 PRECEDING AND 1 PRECEDING) avg_logerror_last_2_month\n",
      "\t, AVG(avg_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING) avg_logerror_last_3_month\n",
      "\t, AVG(std_dev_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) avg_std_dev_logerror_last_1_month\n",
      "\t, AVG(std_dev_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 2 PRECEDING AND 1 PRECEDING) avg_std_dev_logerror_last_2_month\n",
      "\t, AVG(std_dev_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING) avg_std_dev_logerror_last_3_month\n",
      "\t, AVG(std_dev_abs_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 1 PRECEDING AND 1 PRECEDING) avg_std_dev_abs_logerror_last_1_month\n",
      "\t, AVG(std_dev_abs_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 2 PRECEDING AND 1 PRECEDING) avg_std_dev_abs_logerror_last_2_month\n",
      "\t, AVG(std_dev_abs_logerror) OVER (ORDER BY year_and_month ASC ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING) avg_std_dev_abs_logerror_last_3_month\n",
      "FROM\n",
      "\ttmp_aggregated_logerror_information\n",
      ";\n",
      "SELECT 21\n",
      "COPY tmp_aggregated_logerror_infomation_and_trend TO '/Users/dai_li/Workspace/personal/Competitions/zillow/data/monthly_transactions_features.csv' DELIMITER ',' CSV HEADER;\n",
      "COPY 21\n",
      "-- this table generates additional features for transactions\n",
      "DROP TABLE IF EXISTS transactions_additional_features;\n",
      "DROP TABLE\n",
      "CREATE TABLE transactions_additional_features as\n",
      "SELECT \n",
      "\tt.parcelid\n",
      "    , t.logerror\n",
      "    , t.transactiondate\n",
      "    , t.year\n",
      "    , t.month\n",
      "    , t.day_of_week\n",
      "    , f.*\n",
      "FROM \n",
      "\ttmp_additional_temporal_information t\n",
      "JOIN\n",
      "\ttmp_aggregated_logerror_infomation_and_trend f\n",
      "ON\n",
      "\tt.year_and_month = f.year_and_month\t\t\n",
      ";\n",
      "SELECT 167888\n",
      "COPY transactions_additional_features TO '/Users/dai_li/Workspace/personal/Competitions/zillow/data/transactions_features.csv' DELIMITER ',' CSV HEADER;\n",
      "COPY 167888\n"
     ]
    }
   ],
   "source": [
    "!psql -d zillow -a -f ../scripts/feature_engineering_time.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_features = pd.read_csv('../data/monthly_transactions_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_and_month</th>\n",
       "      <th>avg_logerror</th>\n",
       "      <th>avg_abs_logerror</th>\n",
       "      <th>std_dev_logerror</th>\n",
       "      <th>std_dev_abs_logerror</th>\n",
       "      <th>avg_logerror_last_1_month</th>\n",
       "      <th>avg_logerror_last_2_month</th>\n",
       "      <th>avg_logerror_last_3_month</th>\n",
       "      <th>avg_std_dev_logerror_last_1_month</th>\n",
       "      <th>avg_std_dev_logerror_last_2_month</th>\n",
       "      <th>avg_std_dev_logerror_last_3_month</th>\n",
       "      <th>avg_std_dev_abs_logerror_last_1_month</th>\n",
       "      <th>avg_std_dev_abs_logerror_last_2_month</th>\n",
       "      <th>avg_std_dev_abs_logerror_last_3_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01</td>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.072695</td>\n",
       "      <td>0.171525</td>\n",
       "      <td>0.156164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-02</td>\n",
       "      <td>0.016082</td>\n",
       "      <td>0.077434</td>\n",
       "      <td>0.198599</td>\n",
       "      <td>0.183584</td>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.171525</td>\n",
       "      <td>0.171525</td>\n",
       "      <td>0.171525</td>\n",
       "      <td>0.156164</td>\n",
       "      <td>0.156164</td>\n",
       "      <td>0.156164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-03</td>\n",
       "      <td>0.009867</td>\n",
       "      <td>0.072044</td>\n",
       "      <td>0.172171</td>\n",
       "      <td>0.156682</td>\n",
       "      <td>0.016082</td>\n",
       "      <td>0.015976</td>\n",
       "      <td>0.015976</td>\n",
       "      <td>0.198599</td>\n",
       "      <td>0.185062</td>\n",
       "      <td>0.185062</td>\n",
       "      <td>0.183584</td>\n",
       "      <td>0.169874</td>\n",
       "      <td>0.169874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-04</td>\n",
       "      <td>0.006605</td>\n",
       "      <td>0.069972</td>\n",
       "      <td>0.166560</td>\n",
       "      <td>0.151292</td>\n",
       "      <td>0.009867</td>\n",
       "      <td>0.012974</td>\n",
       "      <td>0.013939</td>\n",
       "      <td>0.172171</td>\n",
       "      <td>0.185385</td>\n",
       "      <td>0.180765</td>\n",
       "      <td>0.156682</td>\n",
       "      <td>0.170133</td>\n",
       "      <td>0.165477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-05</td>\n",
       "      <td>0.006926</td>\n",
       "      <td>0.066241</td>\n",
       "      <td>0.150861</td>\n",
       "      <td>0.135716</td>\n",
       "      <td>0.006605</td>\n",
       "      <td>0.008236</td>\n",
       "      <td>0.010851</td>\n",
       "      <td>0.166560</td>\n",
       "      <td>0.169365</td>\n",
       "      <td>0.179110</td>\n",
       "      <td>0.151292</td>\n",
       "      <td>0.153987</td>\n",
       "      <td>0.163853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year_and_month  avg_logerror  avg_abs_logerror  std_dev_logerror  \\\n",
       "0        2016-01      0.015870          0.072695          0.171525   \n",
       "1        2016-02      0.016082          0.077434          0.198599   \n",
       "2        2016-03      0.009867          0.072044          0.172171   \n",
       "3        2016-04      0.006605          0.069972          0.166560   \n",
       "4        2016-05      0.006926          0.066241          0.150861   \n",
       "\n",
       "   std_dev_abs_logerror  avg_logerror_last_1_month  avg_logerror_last_2_month  \\\n",
       "0              0.156164                        NaN                        NaN   \n",
       "1              0.183584                   0.015870                   0.015870   \n",
       "2              0.156682                   0.016082                   0.015976   \n",
       "3              0.151292                   0.009867                   0.012974   \n",
       "4              0.135716                   0.006605                   0.008236   \n",
       "\n",
       "   avg_logerror_last_3_month  avg_std_dev_logerror_last_1_month  \\\n",
       "0                        NaN                                NaN   \n",
       "1                   0.015870                           0.171525   \n",
       "2                   0.015976                           0.198599   \n",
       "3                   0.013939                           0.172171   \n",
       "4                   0.010851                           0.166560   \n",
       "\n",
       "   avg_std_dev_logerror_last_2_month  avg_std_dev_logerror_last_3_month  \\\n",
       "0                                NaN                                NaN   \n",
       "1                           0.171525                           0.171525   \n",
       "2                           0.185062                           0.185062   \n",
       "3                           0.185385                           0.180765   \n",
       "4                           0.169365                           0.179110   \n",
       "\n",
       "   avg_std_dev_abs_logerror_last_1_month  \\\n",
       "0                                    NaN   \n",
       "1                               0.156164   \n",
       "2                               0.183584   \n",
       "3                               0.156682   \n",
       "4                               0.151292   \n",
       "\n",
       "   avg_std_dev_abs_logerror_last_2_month  \\\n",
       "0                                    NaN   \n",
       "1                               0.156164   \n",
       "2                               0.169874   \n",
       "3                               0.170133   \n",
       "4                               0.153987   \n",
       "\n",
       "   avg_std_dev_abs_logerror_last_3_month  \n",
       "0                                    NaN  \n",
       "1                               0.156164  \n",
       "2                               0.169874  \n",
       "3                               0.165477  \n",
       "4                               0.163853  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 14)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine existing data with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_train\n",
    "t['year_and_month'] = t['transactiondate'].str[:7]\n",
    "t = pd.merge(left=t, right=d_features, on=['year_and_month'], how = 'inner', suffixes=('', '_y'))\n",
    "del t['year_and_month']\n",
    "df_train = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred_2016 = pd.merge(sample_submission[['ParcelId']], df_properties_2016.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "#                 how = 'left', on = 'ParcelId')\n",
    "# df_pred_2017 = pd.merge(sample_submission[['ParcelId']], df_properties_2017.rename(columns = {'parcelid': 'ParcelId'}), \n",
    "#                 how = 'left', on = 'ParcelId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the cleaned data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('../tmp/train_full.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_2016.to_csv('../tmp/pred_2016.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_2017.to_csv('../tmp/pred_2017.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fields = set(df.columns)\n",
    "\n",
    "# these are fields that are used to identify fields\n",
    "identifiers = set(['transactiondate', 'parcelid'])\n",
    "\n",
    "# log error that we want to model\n",
    "label = set(['logerror'])\n",
    "\n",
    "# the following are categorical features\n",
    "feats_objects = set(\n",
    " ['taxdelinquencyflag',\n",
    " 'propertycountylandusecode',\n",
    " 'propertyzoningdesc',\n",
    " 'fireplaceflag',\n",
    " 'hashottuborspa']\n",
    ")\n",
    "\n",
    "# the following are numerical features that should be treated as categorical features\n",
    "feats_categorical_as_numeric = set([\n",
    "    'airconditioningtypeid',\n",
    "    'architecturalstyletypeid',\n",
    "    'buildingqualitytypeid',\n",
    "    'buildingclasstypeid',\n",
    "    'decktypeid',\n",
    "    'fips',\n",
    "    'heatingorsystemtypeid',\n",
    "    'propertylandusetypeid',\n",
    "    'regionidcounty',\n",
    "    'regionidcity',\n",
    "    'regionidzip',\n",
    "    'regionidneighborhood',\n",
    "    'storytypeid',\n",
    "    'typeconstructiontypeid',\n",
    "])\n",
    "\n",
    "\n",
    "# the rest are numeric features\n",
    "feats_numeric = set([\n",
    "    'basementsqft',\n",
    "    'bathroomcnt',\n",
    "    'bedroomcnt',\n",
    "    'calculatedbathnbr',\n",
    "    'threequarterbathnbr',\n",
    "    'finishedfloor1squarefeet',\n",
    "    'calculatedfinishedsquarefeet',\n",
    "    'finishedsquarefeet6',\n",
    "    'finishedsquarefeet12',\n",
    "    'finishedsquarefeet13',\n",
    "    'finishedsquarefeet15',\n",
    "    'finishedsquarefeet50',\n",
    "    'fireplacecnt',\n",
    "    'fullbathcnt',\n",
    "    'garagecarcnt',\n",
    "    'garagetotalsqft',\n",
    "    'hashottuborspa',\n",
    "    'lotsizesquarefeet',\n",
    "    'numberofstories',\n",
    "    'poolcnt',\n",
    "    'poolsizesum',\n",
    "    'pooltypeid10',\n",
    "    'pooltypeid2',\n",
    "    'pooltypeid7',\n",
    "    'roomcnt',\n",
    "    'unitcnt',\n",
    "    'yardbuildingsqft17',\n",
    "    'yardbuildingsqft26',\n",
    "    'taxvaluedollarcnt',\n",
    "    'structuretaxvaluedollarcnt',\n",
    "    'landtaxvaluedollarcnt',\n",
    "    'taxamount',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'yearbuilt',\n",
    "    'assessmentyear',\n",
    "    'taxdelinquencyyear',\n",
    "    'rawcensustractandblock',\n",
    "    'censustractandblock',\n",
    "])\n",
    "\n",
    "feats_numerics_feature_engineered = set([col for col in df.columns if 'f_num' in col or '_logerror' in col])\n",
    "\n",
    "\n",
    "feats_categorical_feature_engineered = set([col for col in df.columns if 'f_cat' in col])\n",
    "\n",
    "\n",
    "# fields that are thrown away for now\n",
    "feats_for_consideration_later = set([\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_categorical = feats_objects | feats_categorical_feature_engineered\n",
    "feats_numeric = feats_numeric | feats_numerics_feature_engineered \n",
    "feats = feats_categorical | feats_numeric | feats_categorical_as_numeric\n",
    "\n",
    "feats_categorical_as_numeric = list(feats_categorical_as_numeric)\n",
    "feats_categorical = list(feats_categorical)\n",
    "feats_numeric = list(feats_numeric)\n",
    "feats = list(feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save results to pick files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(feats_categorical_as_numeric, open('../tmp/feats_categorical_as_numeric.pkl', 'w'))\n",
    "pickle.dump(feats_categorical, open('../tmp/feats_categorical.pkl', 'w'))\n",
    "pickle.dump(feats_numeric, open('../tmp/feats_numeric.pkl', 'w'))\n",
    "pickle.dump(feats, open('../tmp/feats.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train = (df.transactiondate < '2019-01-01')\n",
    "mask_validation = (df.transactiondate >= '2016-10-01') & (df.transactiondate < '2017-01-01')\n",
    "# mask_prediction = ~df.parcelid.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mask_train, open('../tmp/mask_train.pkl', 'w'))\n",
    "pickle.dump(mask_validation, open('../tmp/mask_validation.pkl', 'w'))\n",
    "# pickle.dump(mask_prediction, open('../tmp/mask_prediction.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
