{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data\n",
    "data is generated from another script with feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_2016 = pd.read_csv('../tmp/pred_2016.csv')\n",
    "df_pred_2017 = pd.read_csv('../tmp/pred_2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../tmp/train_full.csv', parse_dates=['transactiondate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load feature names\n",
    "feats_numeric = pickle.load(open('../tmp/feats_numeric.pkl', 'r'))\n",
    "feats_categorical_as_numeric = pickle.load(open('../tmp/feats_categorical_as_numeric.pkl', 'r'))\n",
    "feats_categorical = pickle.load(open('../tmp/feats_categorical.pkl', 'r'))\n",
    "feats = pickle.load(open('../tmp/feats.pkl', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train/validation/prediction labels\n",
    "mask_train = pickle.load(open('../tmp/mask_train.pkl', 'r'))\n",
    "mask_test = pickle.load(open('../tmp/mask_validation.pkl', 'r'))\n",
    "# mask_prediction = pickle.load(open('../tmp/mask_prediction.pkl', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute missing values (Non linear models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to impute all these with a `missing` value first\n",
    "df[feats_categorical_as_numeric] = df[feats_categorical_as_numeric].fillna(-1)\n",
    "df_pred_2016[feats_categorical_as_numeric] = df_pred_2016[feats_categorical_as_numeric].fillna(-1)\n",
    "df_pred_2017[feats_categorical_as_numeric] = df_pred_2017[feats_categorical_as_numeric].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use label encoder to encode all categorical variables\n",
    "for column in feats_categorical:\n",
    "    tmp = pd.concat([df[column], df_pred_2016[column], df_pred_2017[column]], axis = 0)\n",
    "    encoder = LabelEncoder().fit(tmp.astype(str))\n",
    "    df[column] = encoder.transform(df[column].astype(str)).astype(np.int32)\n",
    "    df_pred_2016[column] = encoder.transform(df_pred_2016[column].astype(str)).astype(np.int32)\n",
    "    df_pred_2017[column] = encoder.transform(df_pred_2017[column].astype(str)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for numeric variables, fill with a big negative number\n",
    "df[feats_numeric] = df[feats_numeric].fillna(-999)\n",
    "\n",
    "feats_numeric_not_contain_timeseries = [x for x in feats_numeric if \"_logerror\" not in x and \"f_num_month\" not in x and \"f_num_quarter\" not in x]\n",
    "\n",
    "df_pred_2016[feats_numeric_not_contain_timeseries] = df_pred_2016[feats_numeric_not_contain_timeseries].fillna(-999)\n",
    "df_pred_2017[feats_numeric_not_contain_timeseries] = df_pred_2017[feats_numeric_not_contain_timeseries].fillna(-999)\n",
    "\n",
    "# imp =  Imputer(missing_values=np.nan, strategy=\"median\", axis=0)\n",
    "# # df[feats_numeric] = imp.fit_transform(df[feats_numeric])\n",
    "# for feat in feats_numeric:\n",
    "#     if df[feat].isnull().sum() < df.shape[0]:\n",
    "#         df[[feat]] = imp.fit_transform(df[[feat]])\n",
    "#     else:\n",
    "#         df[feat] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate training testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train.reset_index(drop=True, inplace = True)\n",
    "mask_test.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.remove('f_num_stddev_logerror')\n",
    "feats.remove('f_num_avg_logerror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = df.loc[mask_train, feats].astype(float).values\n",
    "# X_test = df.loc[mask_test, feats].astype(float).values\n",
    "\n",
    "# some extra filtering to remove outliers\n",
    "mask_outlier = (df.logerror > 0.4) | (df.logerror < -0.4)\n",
    "# mask_outlier = df.logerror > 999\n",
    "\n",
    "X_train = df.loc[(mask_train & ~mask_outlier), feats].astype(float).values\n",
    "X_test = df.loc[mask_test, feats].astype(float).values\n",
    "\n",
    "y_train = np.array(df.loc[(mask_train & ~mask_outlier), 'logerror'].tolist())\n",
    "y_test = np.array(df.loc[mask_test, 'logerror'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {}\n",
    "# params['max_bin'] = 10\n",
    "# params['learning_rate'] = 0.0021 # shrinkage_rate\n",
    "# params['boosting_type'] = 'gbdt'\n",
    "# params['objective'] = 'regression'\n",
    "# params['metric'] = 'mae'          # or 'mae'\n",
    "# params['sub_feature'] = 0.345    \n",
    "# params['bagging_fraction'] = 0.85 # sub_row\n",
    "# params['bagging_freq'] = 40\n",
    "# params['num_leaves'] = 512        # num_leaf\n",
    "# params['min_data'] = 500         # min_data_in_leaf\n",
    "# params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n",
    "# params['verbose'] = 0\n",
    "# params['feature_fraction_seed'] = 2\n",
    "# params['bagging_seed'] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General parameter tuning guide:\n",
    "For Better Accuracy\n",
    "Use large max_bin (may be slower)\n",
    "Use small learning_rate with large num_iterations\n",
    "Use large num_leaves(may cause over-fitting)\n",
    "Use bigger training data\n",
    "Try dart\n",
    "\n",
    "\n",
    "Deal with Over-fitting\n",
    "Use small max_bin\n",
    "Use small num_leaves\n",
    "Use min_data_in_leaf and min_sum_hessian_in_leaf\n",
    "Use bagging by set bagging_fraction and bagging_freq\n",
    "Use feature sub-sampling by set feature_fraction\n",
    "Use bigger training data\n",
    "Try lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "Try max_depth to avoid growing deep tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # 'max_bin': [10],\n",
    "    'max_depth': [100],\n",
    "    'num_leaves': [32],\n",
    "    'feature_fraction': [0.85],\n",
    "    'bagging_fraction': [0.95],\n",
    "    'bagging_freq': [8],\n",
    "    'learning_rate': [0.0025],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV] num_leaves=32, bagging_freq=8, learning_rate=0.0025, bagging_fraction=0.95, max_depth=100, feature_fraction=0.85 \n",
      "[CV] num_leaves=32, bagging_freq=8, learning_rate=0.0025, bagging_fraction=0.95, max_depth=100, feature_fraction=0.85 \n",
      "[CV] num_leaves=32, bagging_freq=8, learning_rate=0.0025, bagging_fraction=0.95, max_depth=100, feature_fraction=0.85 \n",
      "[CV]  num_leaves=32, bagging_freq=8, learning_rate=0.0025, bagging_fraction=0.95, max_depth=100, feature_fraction=0.85, total=  10.7s\n",
      "[CV]  num_leaves=32, bagging_freq=8, learning_rate=0.0025, bagging_fraction=0.95, max_depth=100, feature_fraction=0.85, total=  11.1s\n",
      "[CV]  num_leaves=32, bagging_freq=8, learning_rate=0.0025, bagging_fraction=0.95, max_depth=100, feature_fraction=0.85, total=  11.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   3 out of   3 | elapsed:   12.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] randomized search took 20.87 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "scorer = make_scorer(mean_absolute_error)\n",
    "model = lgb.LGBMRegressor(boosting_type = 'gbdt', \n",
    "                          metric='mae', \n",
    "                          verbosity=0, \n",
    "                          #verbose_eval=200, \n",
    "                          num_boost_round=200,\n",
    "                          #bagging_seed=42, \n",
    "                          #feature_fraction_seed=42,\n",
    "                          n_jobs = 6\n",
    "                          )\n",
    "grid = GridSearchCV(model, params, scoring=scorer,\n",
    "                          cv=3, verbose=2, n_jobs=6)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"[INFO] randomized search took {:.2f} seconds\".format(\n",
    "    time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for test set and prediction set\n",
    "y_pred = grid.predict(X_test)\n",
    "# pred_lgb = grid.predict(X_pred_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] grid search MAE: 0.065371\n",
      "[INFO] randomized search best parameters: {'num_leaves': 32, 'bagging_fraction': 0.95, 'learning_rate': 0.0025, 'bagging_freq': 8, 'max_depth': 100, 'feature_fraction': 0.85}\n"
     ]
    }
   ],
   "source": [
    "mae = grid.score(X_test, y_test)\n",
    "print(\"[INFO] grid search MAE: {:.6f}\".format(mae))\n",
    "print(\"[INFO] randomized search best parameters: {}\".format(\n",
    "    grid.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] grid search MAE: 0.064093\n",
      "[INFO] randomized search best parameters: {'num_leaves': 32, 'bagging_fraction': 0.95, 'learning_rate': 0.0025, 'bagging_freq': 8, 'max_depth': 100, 'feature_fraction': 0.85}\n"
     ]
    }
   ],
   "source": [
    "mae = grid.score(X_test, y_test)\n",
    "print(\"[INFO] grid search MAE: {:.6f}\".format(mae))\n",
    "print(\"[INFO] randomized search best parameters: {}\".format(\n",
    "    grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "results['ParcelId'] = df_pred_2016['ParcelId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now doing prediction for 201610 ...\n",
      "now doing prediction for 201611 ...\n",
      "now doing prediction for 201612 ...\n",
      "now doing prediction for 201710 ...\n",
      "now doing prediction for 201711 ...\n",
      "now doing prediction for 201712 ...\n"
     ]
    }
   ],
   "source": [
    "df_train = df\n",
    "#for m in [201610]:\n",
    "for m in [201610, 201611, 201612, 201710, 201711, 201712]:\n",
    "    print \"now doing prediction for {} ...\".format(str(m))\n",
    "    year = str(m)[:4]\n",
    "    month = str(m)[4:]\n",
    "    \n",
    "    # avg log error, monthly\n",
    "    group = df_train.groupby(df_train.f_num_month)['logerror'].aggregate('mean').to_dict()\n",
    "    eval('df_pred_'+year)['f_num_monthly_avg_logerror'] = group[int(month)]\n",
    "                                   \n",
    "    # std dev log error, monthly\n",
    "    group = df_train.groupby(df_train.f_num_month)['logerror'].aggregate('std').to_dict()\n",
    "    eval('df_pred_'+year)['f_num_monthly_stddev_logerror'] = group[int(month)]\n",
    "\n",
    "    # avg log error, quarterly\n",
    "    group = df_train.groupby(df_train.f_num_month)['logerror'].aggregate('mean').to_dict()\n",
    "    eval('df_pred_'+year)['f_num_quarterly_avg_logerror'] = group[4]\n",
    "    \n",
    "    # std dev log error, monthly\n",
    "    group = df_train.groupby(df_train.f_num_month)['logerror'].aggregate('std').to_dict()\n",
    "    eval('df_pred_'+year)['f_num_quarterly_stddev_logerror'] = group[4]\n",
    "    \n",
    "    \n",
    "    eval('df_pred_'+year)['f_num_month'] = int(month)\n",
    "    eval('df_pred_'+year)['f_num_quarter'] = 4\n",
    "    \n",
    "    pred = grid.predict(eval('df_pred_'+year)[feats].astype(float).values)\n",
    "    results[str(m)] = pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape[0] == 2985217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('../tmp/submission_{}.csv'.format(datetime.datetime.now().strftime('%Y%m%d_%H%M%S')), index=False, float_format = '%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost params\n",
    "params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': np.mean(y_train),\n",
    "    'silent': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = xgb.DMatrix(X_train, y_train)\n",
    "d_test = xgb.DMatrix(X_test)\n",
    "d_pred = xgb.DMatrix(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boost_rounds = 150\n",
    "fit = xgb.train(dict(params, silent=1), d_train, num_boost_round=num_boost_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for test set and prediction set\n",
    "y_pred = fit.predict(d_test)\n",
    "pred_xgb = fit.predict(d_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.076796\n"
     ]
    }
   ],
   "source": [
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.076796\n"
     ]
    }
   ],
   "source": [
    "# when not using oct data\n",
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\": 10,\n",
    "    \"min_samples_split\": 50,\n",
    "    \"n_estimators\": 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   29.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=10,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=50,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1,\n",
       "           oob_score=False, random_state=42, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit = RandomForestRegressor(random_state=42, n_jobs=-1, verbose=1, **params)\n",
    "fit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:   14.9s finished\n"
     ]
    }
   ],
   "source": [
    "# prediction for test set and prediction set\n",
    "y_pred = fit.predict(X_test)\n",
    "pred_rf = fit.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.076814\n"
     ]
    }
   ],
   "source": [
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.076814\n"
     ]
    }
   ],
   "source": [
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Elastic Net\n",
    "For linear model, need to have different missing value imputation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'l1_ratio':0.7, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,\n",
       "       l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=-1,\n",
       "       normalize=False, positive=False, precompute='auto', random_state=0,\n",
       "       selection='cyclic', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit = ElasticNetCV(cv=5, random_state=42)\n",
    "fit.fit(X_train, y_train)\n",
    "ElasticNetCV(alphas=None, copy_X=True, cv=5, eps=0.001, fit_intercept=True,\n",
    "       l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=-1,\n",
    "       normalize=False, positive=False, precompute='auto', random_state=0,\n",
    "       selection='cyclic', tol=0.0001, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for test set and prediction set\n",
    "y_pred = fit.predict(X_test)\n",
    "#pred_enet = fit.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_test_set_performance():\n",
    "    #explained_variance_score(y_pred=y_pred, y_true=y_test)\n",
    "    print 'MAE:{}'.format(round(mean_absolute_error(y_pred=y_pred, y_true=y_test),6))\n",
    "    # pd.Series(pred_lgb_test).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.06579\n"
     ]
    }
   ],
   "source": [
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature_inds = []\n",
    "cat_unique_thresh = 1000\n",
    "for i, c in enumerate(feats_categorical):\n",
    "    num_uniques = len(df[c].unique())\n",
    "    if num_uniques < cat_unique_thresh \\\n",
    "       and not 'sqft' in c \\\n",
    "       and not 'cnt' in c \\\n",
    "       and not 'nbr' in c \\\n",
    "       and not 'number' in c:\n",
    "        cat_feature_inds.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [04:13<00:00, 50.63s/it]\n"
     ]
    }
   ],
   "source": [
    "num_ensembles = 5\n",
    "y_pred = 0.0\n",
    "for i in tqdm(range(num_ensembles)):\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=300, learning_rate=0.03,\n",
    "        depth=6, l2_leaf_reg=3,\n",
    "        loss_function='MAE',\n",
    "        eval_metric='MAE',\n",
    "        random_seed=i)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        #cat_features=cat_feature_inds\n",
    "    )\n",
    "    y_pred += model.predict(X_test)\n",
    "y_pred /= num_ensembles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.063277\n"
     ]
    }
   ],
   "source": [
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:0.06418\n"
     ]
    }
   ],
   "source": [
    "report_test_set_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensenmble - stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = {\n",
    "    'lgb': 1,\n",
    "    'xgb': 0.00,\n",
    "    'rf': 0.00,\n",
    "    'enet': 0,\n",
    "    'baseline': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions first for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_all = 0\n",
    "pred_baseline = df.logerror.mean()\n",
    "for model_name, model_weight in model_weights.iteritems():\n",
    "    pred_all += eval('pred_'+model_name) * model_weight\n",
    "pred_all = pred_all / sum(model_weights.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = df[['parcelid','logerror']]\n",
    "df_submission['pred'] = pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_submission['201610'] = df_submission['logerror'].combine_first(df_submission['pred'])\n",
    "df_submission['201610'] = df_submission['pred']\n",
    "df_submission['201611'] = df_submission['pred']\n",
    "df_submission['201612'] = df_submission['pred']\n",
    "df_submission['201710'] = df_submission['pred']\n",
    "df_submission['201711'] = df_submission['pred']\n",
    "df_submission['201712'] = df_submission['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.drop_duplicates(subset = ['parcelid'], keep='first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_submission['logerror']\n",
    "del df_submission['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.shape[0] == 2985217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('../tmp/submission_{}.csv'.format(datetime.datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
